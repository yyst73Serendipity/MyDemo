"""
AIè§’è‰²Promptè¯„æµ‹ç³»ç»Ÿ - ä¸»ç¨‹åº
ç”¨äºè‡ªåŠ¨åŒ–æµ‹è¯•å¤šä¸ªAIæ¨¡å‹å¯¹è§’è‰²æ‰®æ¼”çš„è¡¨ç°
"""

import os
import json
import yaml
from datetime import datetime
from typing import Dict, List, Any

from model_clients import ModelClientFactory, test_client
from evaluator import ResponseEvaluator
from report_generator import ReportGenerator
from logger import DualLogger


class PromptEvaluationSystem:
    """Promptè¯„æµ‹ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, logger: DualLogger, config_path: str = "config.yaml"):
        """åˆå§‹åŒ–è¯„æµ‹ç³»ç»Ÿ"""
        self.logger = logger
        self.logger.print("ğŸš€ åˆå§‹åŒ–AIè§’è‰²Promptè¯„æµ‹ç³»ç»Ÿ...")
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # åŠ è½½è§’è‰²prompt
        self.system_prompt = self._load_prompt_template()
        
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        self.test_cases = self._load_test_cases()
        
        # åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
        self.logger.print("\nğŸ“¦ æ­£åœ¨åŠ è½½AIæ¨¡å‹å®¢æˆ·ç«¯...")
        
        # è¯»å– DMXAPI ç»Ÿä¸€é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        dmxapi_config = self.config.get('dmxapi', None)
        if dmxapi_config:
            self.logger.print(f"  ğŸ”— ä½¿ç”¨ DMXAPI èšåˆå¹³å°: {dmxapi_config.get('base_url', 'N/A')}")
        
        self.clients = ModelClientFactory.create_all_clients(
            self.config['models'], 
            self.logger, 
            dmxapi_config
        )
        
        if not self.clients:
            raise RuntimeError("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡å‹å®¢æˆ·ç«¯ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # åˆ›å»ºè¯„ä¼°å™¨å’ŒæŠ¥å‘Šç”Ÿæˆå™¨
        self.evaluator = ResponseEvaluator()
        
        raw_responses_dir = self.config['output']['raw_responses_dir']
        reports_dir = self.config['output']['reports_dir']
        
        os.makedirs(raw_responses_dir, exist_ok=True)
        os.makedirs(reports_dir, exist_ok=True)
        
        self.raw_responses_dir = raw_responses_dir
        self.report_generator = ReportGenerator(reports_dir)
        
        self.logger.print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        self.logger.print(f"   - å·²åŠ è½½ {len(self.clients)} ä¸ªæ¨¡å‹")
        self.logger.print(f"   - å·²åŠ è½½ {len(self.test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _load_prompt_template(self) -> str:
        """åŠ è½½è§’è‰²promptæ¨¡æ¿"""
        with open('prompt_template.txt', 'r', encoding='utf-8') as f:
            return f.read()
    
    def _load_test_cases(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        with open('test_cases.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data['test_cases']
    
    def run_evaluation(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹"""
        self.logger.print("\n" + "="*60)
        self.logger.print("ğŸ¯ å¼€å§‹æ‰§è¡Œè¯„æµ‹...")
        self.logger.print("="*60 + "\n")
        
        all_results = {}
        
        # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œæµ‹è¯•
        for model_name, client in self.clients.items():
            self.logger.print(f"\n{'='*60}")
            self.logger.print(f"ğŸ¤– æ­£åœ¨æµ‹è¯•æ¨¡å‹: {model_name}")
            self.logger.print(f"{'='*60}\n")
            
            model_results = {}
            
            # å¯¹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹è¿›è¡Œæµ‹è¯•
            for i, test_case in enumerate(self.test_cases, 1):
                test_id = test_case['id']
                self.logger.print(f"\n[{i}/{len(self.test_cases)}] æµ‹è¯•ç”¨ä¾‹ {test_id}: {test_case['category']}")
                self.logger.print(f"  ğŸ“ è¾“å…¥: {test_case['input'][:50]}...")
                
                # è°ƒç”¨æ¨¡å‹
                result = test_client(client, self.system_prompt, test_case['input'], self.logger)
                
                if result:
                    response_text = result['content']
                    
                    # ä¿å­˜åŸå§‹å“åº”
                    self._save_raw_response(model_name, test_id, test_case, result)
                    
                    # è‡ªåŠ¨è¯„ä¼°
                    evaluation = self.evaluator.evaluate_response(test_case, response_text)
                    
                    # æ˜¾ç¤ºè¯„åˆ†ï¼ˆä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„ï¼‰
                    self.logger.print(f"  ğŸ“Š è¯„ä¼°å¾—åˆ†: {evaluation['raw_total']}/{evaluation['raw_max']} (åŸå§‹åˆ†)")
                    self.logger.print(f"  ğŸ¯ åŠ æƒæ€»åˆ†: {evaluation['total_score_100']:.1f}/100")
                    self.logger.print(f"  âœ… æµ‹è¯•å®Œæˆ")
                    
                    model_results[test_id] = {
                        'response': response_text,
                        'evaluation': evaluation,
                        'raw_result': result
                    }
                else:
                    self.logger.print(f"  âŒ è°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
                    model_results[test_id] = {
                        'response': None,
                        'evaluation': None,
                        'error': 'APIè°ƒç”¨å¤±è´¥'
                    }
            
            all_results[model_name] = model_results
        
        return all_results
    
    def _save_raw_response(self, model_name: str, test_id: str, 
                          test_case: Dict[str, Any], result: Dict[str, Any]) -> None:
        """ä¿å­˜åŸå§‹å“åº”åˆ°JSONæ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # æ¸…ç†æ¨¡å‹åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆæ›¿æ¢ / ä¸º _ï¼‰
        safe_model_name = model_name.replace('/', '_').replace('\\', '_')
        filename = f"{safe_model_name}_{test_id}_{timestamp}.json"
        filepath = os.path.join(self.raw_responses_dir, filename)
        
        data = {
            "model": model_name,
            "test_case_id": test_id,
            "test_category": test_case['category'],
            "timestamp": timestamp,
            "input": test_case['input'],
            "intent": test_case['intent'],
            "response": result['content'],
            "raw_api_response": result.get('raw', {}),
            "usage": result.get('usage', {})
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def generate_report(self, test_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        self.logger.print("\n" + "="*60)
        self.logger.print("ğŸ“„ æ­£åœ¨ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        self.logger.print("="*60 + "\n")
        
        report_path = self.report_generator.generate_report(test_results, self.test_cases)
        
        self.logger.print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # æ‰“å°ç®€å•æ‘˜è¦
        summary = self.report_generator.generate_simple_summary(test_results)
        self.logger.print(summary)
        
        return report_path


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = DualLogger(log_dir="results/logs", enable_console=True)
    
    try:
        # åˆ›å»ºè¯„æµ‹ç³»ç»Ÿå®ä¾‹
        system = PromptEvaluationSystem(logger)
        
        # è¿è¡Œè¯„æµ‹
        results = system.run_evaluation()
        
        # ç”ŸæˆæŠ¥å‘Š
        report_path = system.generate_report(results)
        
        logger.print("\n" + "="*60)
        logger.print("ğŸ‰ è¯„æµ‹å®Œæˆï¼")
        logger.print("="*60)
        logger.print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Š: {report_path}")
        logger.print(f"ğŸ“ åŸå§‹å“åº”: {system.raw_responses_dir}")
        logger.print(f"ğŸ“‹ æ‰§è¡Œæ—¥å¿—: {logger.get_log_path()}")
        logger.print("\nğŸ’¡ æç¤º: è¯·æŸ¥çœ‹æŠ¥å‘Šå¹¶å¡«å†™äººå·¥è¯„ä»·éƒ¨åˆ†\n")
        
    except FileNotFoundError as e:
        logger.print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        logger.print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        logger.print("  - config.yaml")
        logger.print("  - prompt_template.txt")
        logger.print("  - test_cases.json")
    except Exception as e:
        logger.print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        # åŒæ—¶å°†tracebackå†™å…¥æ—¥å¿—
        import io
        trace_str = io.StringIO()
        traceback.print_exc(file=trace_str)
        logger.print(trace_str.getvalue())
    finally:
        # å…³é—­æ—¥å¿—æ–‡ä»¶
        logger.close()


if __name__ == "__main__":
    main()

